%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=12pt]{article}% A4 paper and 11pt font size

\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[]{mcode} %Matlab code package
%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{	
\textsc{Artificial Neural Networks} \\ [25pt] % Your university, school and/or department name(s)
\huge Exercise Set 1 \\ % The assignment title
}

\author{Antonio Peters} % Your name

\date{\today} % Today's date or a custom date


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%DOCUMENT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle % Print the title

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 1.1.1}
%----------------------------------------------------------------------
\subsection{[1] (a)}
$f(x) = ax^2+bx+c$ \\
Let $\bar{v} = [ax_1^2+bx_1+c,...,ax_n^2+bx_n+c]$ \\
Let $\bar{y} = [y_1,y_2,...,y_n]$ \\
Minimize $\bar{e} = \bar{v} - \bar{y} = [(ax_1^2+bx_1+c-y_1),...,(ax_n^2+bx_n+c-y_n)]$ \\
We need to minimize $||\bar{e}||$ or equivalently minimize $S = ||\bar{e}||^2$. \\
Therefore $S = \sum_{i=1}^{n}(x_i^2+bx_i+c-y_i)^2$,the sum of square errors needs to be minimized.\\

Set: \\
\begin{equation}
	\begin{aligned}
		& \qquad \frac{\partial S}{\partial a} = \sum_{i=1}^{n} 2(ax_i^2+bx_i+c-y_i)x_i^2 = 0 \\
		& \qquad \frac{\partial S}{\partial b} = \sum_{i=1}^{n} 2(ax_i^2+bx_i+c-y_i)x_i = 0 \\
		& \qquad \frac{\partial S}{\partial c} = \sum_{i=1}^{n} 2(ax_i^2+bx_i+c-y_i) = 0
	\end{aligned}
\end{equation}

\pagebreak

Which is equivalent to: \\
\begin{equation}
	\begin{aligned}
		& \qquad (\sum_{i=1}^{n}x_i^4)a	+ (\sum_{i=1}^{n}x_i^3)b 	+ (\sum_{i=1}^{n}x_i^2)c = \sum_{i=1}^{n}x_i^2y_i \\
		& \qquad (\sum_{i=1}^{n}x_i^3)a	+ (\sum_{i=1}^{n}x_i^2)b 	+ (\sum_{i=1}^{n}x_i)c = \sum_{i=1}^{n}x_iy_i \\
		& \qquad (\sum_{i=1}^{n}x_i^2)a	+ (\sum_{i=1}^{n}x_i)b 		+ \sum_{i=1}^{n}c = \sum_{i=1}^{n}y_i
	\end{aligned}
\end{equation}

Which becomes: \\

\begin{equation}
	\begin{bmatrix}
	    \sum_{i=1}^{n}x_i^4       & \sum_{i=1}^{n}x_i^3 	& \sum_{i=1}^{n}x_i^2 \\
	    \sum_{i=1}^{n}x_i^3       & \sum_{i=1}^{n}x_i^2 	& \sum_{i=1}^{n}x_i \\
	    \sum_{i=1}^{n}x_i^2       & \sum_{i=1}^{n}x_i 		& n
	\end{bmatrix}
	\begin{bmatrix}
	   a \\
	   b \\
	   c
	\end{bmatrix}
	=
	\begin{bmatrix}
	    \sum_{i=1}^{n}x_i^2y_i \\
	    \sum_{i=1}^{n}x_iy_i \\
	    \sum_{i=1}^{n}y_i
	\end{bmatrix}
\end{equation}

Which can be represented by: \\
\begin{equation}
	\begin{aligned}
		\qquad A\bar{b} = C
	\end{aligned}
\end{equation}

And our coefficients can be found by solving: \\
\begin{equation}
	\begin{aligned}
		\qquad \bar{b} = C/A
	\end{aligned}
\end{equation}
%--------------------------------------------------------------------
\subsection{[1] (b)}
See q1\_1\_b.m and quadreg.m
%--------------------------------------------------------------------
\subsection{[2] (a)}
$f(x) = M(1-e^{-kx})$ \\
Let $\bar{v} = [M(1-e^{-kx_1}),...,M(1-e^{-kx_n})]$ \\
Let $\bar{y} = [y_1,y_2,...,y_n]$ \\
Minimize $\bar{e} = \bar{v} - \bar{y} = [(M(1-e^{-kx_1})-y_1),...,(M(1-e^{-kx_n})-y_n)]$ \\
We need to minimize $||\bar{e}||$ or equivalently minimize $S = ||\bar{e}||^2$. \\
Therefore $S = \sum_{i=1}^{n}(M(1-e^{-kx_i})-y_i)^2$,the sum of square errors needs to be minimized.\\

Set: \\
\begin{equation}
	\begin{aligned}
		& \qquad \frac{\partial S}{\partial M} = \sum_{i=1}^{n} 2(M(1-e^{-kx_i})-y_i)(1-e^{-kx_i}) = 0 \\
		& \qquad \frac{\partial S}{\partial k} = \sum_{i=1}^{n} 2(M(1-e^{-kx_i})-y_i)Mx_ie^{-kx_i} = 0 \\
	\end{aligned}
\end{equation}

Which is equivalent to: \\
\begin{equation}
	\begin{aligned}
		& \qquad \sum_{i=1}^{n} (M-Me^{-kx_i}-y_i) (1-e^{-kx_i}) = 0\\
		& \qquad \sum_{i=1}^{n} (M-Me^{-kx_i}-y_i) Mx_ie^{-kx_i} = 0\\
	\end{aligned}
\end{equation}

Which can be simplified to: \\
\begin{equation}
	\begin{aligned}
		& \qquad \sum_{i=1}^{n} M(1-2e^{-kx_i}-e^{-2kx_i}) = y_ie^{-kx_i} + y_i\\
		& \qquad \sum_{i=1}^{n} M - Me^{-kx_i} = y_i\\
	\end{aligned}
\end{equation}

M and k are clearly not separable to separate the equations into a set of matrix equations, therefore, there is no explicit solution.

%--------------------------------------------------------------------
\subsection{[2] (b and c)}
See q1\_2\_b.m
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ }
See q2.m and r2.m \\

$q1\_1\_b.m \rightarrow r2 = 0.996$ \\

$q1\_2\_b.m \rightarrow r2 = 1$\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 1.6.1}

See layer1.m

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 2.1.1}
%--------------------------------------------------------------------
\subsection{[1]}
We seek: \\
\begin{equation}
	\begin{bmatrix}
	    0	&0	&1	&1\\
	    0	&1	&0	&1
	\end{bmatrix}
	\rightarrow
	\begin{bmatrix}
	    0	&0	&0	&1
	\end{bmatrix}
\end{equation}

Therefore, we need: \\
\begin{equation}
	\begin{aligned}
		(0,0) \rightarrow 0 \\
		(0,1) \rightarrow 0 \\
		(1,0) \rightarrow 0 \\
		(1,1) \rightarrow 1 \\
	\end{aligned}
\end{equation}

Which can be derived to: \\
\begin{equation}
	\begin{aligned}
		hardlim(W \binom{0}{0}+ b) = 0 \\
		hardlim(W \binom{0}{1}+ b) = 0 \\
		hardlim(W \binom{1}{0}+ b) = 0 \\
		hardlim(W \binom{1}{1}+ b) = 1 \\
	\end{aligned}
\end{equation}
or: \\
\begin{equation}
	\begin{aligned}
		W \binom{0}{0}+ b < 0 \\
		W \binom{0}{1}+ b < 0 \\
		W \binom{1}{0}+ b < 0 \\
		W \binom{1}{1}+ b > 0 \\
	\end{aligned}
\end{equation}

Assuming a Weighting of $(1,1)$, any bias between 1 and 2 will give us the needed result, in this case, a bias of $1.1$ was used.
See q4.m for results.
%--------------------------------------------------------------------
\subsection{[2]}
We seek: \\
\begin{equation}
	\begin{bmatrix}
	    0	&0	&1	&1\\
	    0	&1	&0	&1
	\end{bmatrix}
	\rightarrow
	\begin{bmatrix}
	    1	&0	&0	&1
	\end{bmatrix}
\end{equation}

Therefore, we need: \\
\begin{equation}
	\begin{aligned}
		(0,0) \rightarrow 1 \\
		(0,1) \rightarrow 0 \\
		(1,0) \rightarrow 0 \\
		(1,1) \rightarrow 1 \\
	\end{aligned}
\end{equation}

Which can be derived to: \\
\begin{equation}
	\begin{aligned}
		hardlim(W \binom{0}{0}+ b) = 1 \\
		hardlim(W \binom{0}{1}+ b) = 0 \\
		hardlim(W \binom{1}{0}+ b) = 0 \\
		hardlim(W \binom{1}{1}+ b) = 1 \\
	\end{aligned}
\end{equation}
or: \\
\begin{equation}
	\begin{aligned}
		W \binom{0}{0}+ b > 0 \\
		W \binom{0}{1}+ b < 0 \\
		W \binom{1}{0}+ b < 0 \\
		W \binom{1}{1}+ b > 0 \\
	\end{aligned}
\end{equation}

Again assuming a weighting of $(1,1)$ there does not exist a bias which will produce this result, this is due to the fact that the problem is not linearly separable, meaning that a straight line does not exist which can separate the data.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ }
%--------------------------------------------------------------------
\subsection{(a)}

\begin{equation}
	\begin{aligned}
		a 	= f(n) 	= tansig(n) 	= \frac{sinh(n)}{cosh(n)} \\
			= \frac{\frac{e^n-e^{-n}}{2}}{\frac{e^n+e^{-n}}{2}} = \frac{e^n-e^{-n}}{e^n+e^{-n}}
	\end{aligned}
\end{equation}

The derivative is therefore:

\begin{equation}
	\begin{aligned}
		\dot{a} = \dot{f}(n) = \frac{d}{dn} \frac{e^n-e^{-n}}{e^n+e^{-n}}
	\end{aligned}
\end{equation}

By the quotient rule:

\begin{equation}
	\begin{aligned}
		\frac{d}{{dx}}\left( {\frac{{f\left( x \right)}}{{g\left( x \right)}}} \right) = \frac{{\frac{d}{{dx}}f\left( x \right)g\left( x \right) - f\left( x \right)\frac{d}{{dx}}g\left( x \right)}}{{g^2 \left( x \right)}}
	\end{aligned}
\end{equation}

This equates to:

\begin{equation}
	\begin{aligned}
		\frac{(e^n+e^{-n})(e^n+e^{-n}) - (e^n-e^{-n})(e^n-e^{-n})}{(e^n+e^{-n})^2} \\
		=	\frac{(e^n+e^{-n})^2 - (e^n-e^{-n})^2}{(e^n+e^{-n})^2} \\
		=	\frac{(e^n+e^{-n})^2}{(e^n+e^{-n})^2} - \frac{(e^n-e^{-n})^2}{(e^n+e^{-n})^2} \\
		= 	1 - a^2
	\end{aligned}
\end{equation}
%--------------------------------------------------------------------
\subsection{(b)}

\begin{equation}
	\begin{aligned}
		a 	= f(n) 	= logsig(n) 	= \frac{1}{1+e^{-n}}
	\end{aligned}
\end{equation}

The derivative is therefore:

\begin{equation}
	\begin{aligned}
		\dot{a} = \dot{f}(n) = \frac{d}{dn} \frac{1}{1+e^{-n}} \\
				= \big{(}- \frac{1}{1+e^{-n}}\big{) (}-e^{-n}\big{)}\\
				= \frac{e^{-n}}{(1+e^{-n})^2}\\
				= \frac{1 + e^{-n} - 1}{(1+e^{-n})^2}\\
				= \frac{1 + e^{-n}}{(1+e^{-n})^2} - \frac{1}{(1+e^{-n})^2}\\
				= \frac{1}{1+e^{-n}} - \frac{1}{(1+e^{-n})^2}\\
				= a - a^2 = (1-a)a
	\end{aligned}
\end{equation}
%--------------------------------------------------------------------
\subsection{(c)}
For a:
\begin{lstlisting} 
	a =@(n) tansig(n);
	dot_f = 1 - a(n)^2;
\end{lstlisting}


For b:
\begin{lstlisting} 
	a =@(n) logsig(n);
	dot_f = (1 - a(n))*a(n);
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%